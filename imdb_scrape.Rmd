---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

# Info 

The current application uses data from the IMDb Charts, which are subject to change. The list itself is called "Popular Movies", and determined by an unidentified method by IMDb Users and sorted by Ranking. The list can be accessed here: https://www.imdb.com/chart/moviemeter/?ref_=nv_mv_mpm

# Outline 

After loading the libraries, the 100 most popular films are scraped by vectorization. The second data chunk is dedicated to retrieving some basic characteristics: rating, metascore, description, scoreline, plot keywords and genres. The third data chunk is dedicated to acquiring the movies' title and year of screening.

```{r message = FALSE}
library(rvest)
library(stringr)
library(dplyr)
library(openxlsx)
```

```{r scrape 100 most popular films based on rating}
# Page 1
# Scrape HTML content from a given URL
url1 <- read_html("https://www.imdb.com/search/title/?year=2020&title_type=feature&")
# Call node based on xpath
nodes1 <- html_nodes(url1, ".lister-item-header a")
# Create IMDb links to scrape through 
sub_urls1 <- html_attr(nodes1, 'href') # extract parts of the link 
m_urls1 <- paste0('http://www.imdb.com', sub_urls1) # Concatenate with part that is missing from link

# Page 2
# Scrape HTML content from a given URL
url2 <- read_html("https://www.imdb.com/search/title/?title_type=feature&year=2020-01-01,2020-12-31&start=51&ref_=adv_nxt")
# Call node based on xpath
nodes2 <- html_nodes(url2, ".lister-item-header a")
# Create IMDb links to scrape through 
sub_urls2 <- html_attr(nodes2, 'href') # extract parts of the link 
m_urls2 <- paste0('http://www.imdb.com', sub_urls2) # Concatenate with part that is missing from link
```

```{r build dataframe for description}
# Function to scrape description 
getdescription <- function(url){
  page <- read_html(url)
  nodes <- html_nodes(page, '.canwrap span , .see-more.canwrap')
  cast <- html_text(nodes, trim = TRUE)
  return(cast)
}

# Use lapply() to apply the getdescription() function to each URL 
desc1 <- lapply(m_urls1, getdescription)
desc2 <- lapply(m_urls2, getdescription)
# Get individual length of lists 
len1 <- sapply(desc1,length)
len2 <- sapply(desc2,length)
# Get longest list 
n1 <- max(len1)
n2 <- max(len2)
# Get number of NA's to fill for lists shorter than longest
lenNA1 <- n1 - len1
lenNA2 <- n2 - len2
# Create a df that fills every list shorter than 11 with NA's
df1 <- data.frame(t(mapply(function(x,y) c(x, rep(NA, y)), desc1, lenNA1)))
df2 <- data.frame(t(mapply(function(x,y) c(x, rep(NA, y)), desc2, lenNA2)))

# Correct inconsistencies 
# Table 1
df1[which(!(grepl("Plot", df1$X2))), 13] <- df1[which(!(grepl("Plot", df1$X2))), 2]
df1[which(!(grepl("Plot", df1$X2))), 2] <- NA
df1[which(grepl("Genres", df1$X5)), 13] <- df1[which(grepl("Genres", df1$X5)), 5]
df1[which(grepl("Genres", df1$X7)), 13] <- df1[which(grepl("Genres", df1$X7)), 7]
df1[which(grepl("Genres", df1$X9)), 13] <- df1[which(grepl("Genres", df1$X9)), 9]
df1[which(grepl("Genres", df1$X11)), 13] <- df1[which(grepl("Genres", df1$X11)), 11]

# Table 2
df2[which(!(grepl("Plot", df2$X2))), 13] <- df2[which(!(grepl("Plot", df2$X2))), 2]
df2[which(!(grepl("Plot", df2$X2))), 2] <- NA
df2[which(grepl("Genres", df2$X5)), 13] <- df2[which(grepl("Genres", df2$X5)), 5]
df2[which(grepl("Genres", df2$X7)), 13] <- df2[which(grepl("Genres", df2$X7)), 7]
df2[which(grepl("Genres", df2$X9)), 13] <- df2[which(grepl("Genres", df2$X9)), 9]


# Cleaning
df1$X2 <- df1$X2 %>% 
  gsub("Plot Keywords:", "", .) %>% 
  gsub("SeeAll\\([0-9]+\\)", "", .) %>% 
  gsub("See All", "", .) %>% 
  gsub("\\»", "", .) %>% 
  gsub("\\([0-9]+\\)", "", .) %>%
  gsub("\\s+"," ", .) %>% 
  gsub(" \\| ", ", ", .)

df1$X13 <- df1$X13 %>% 
  gsub("Genres:\n", "", .) %>% 
  gsub("\\s+"," ", .) %>% 
  gsub(" \\| ", ", ", .) %>% 
  gsub("\\|", "", .)

df2$X2 <- df2$X2 %>% 
  gsub("Plot Keywords:", "", .) %>% 
  gsub("SeeAll\\([0-9]+\\)", "", .) %>% 
  gsub("See All", "", .) %>% 
  gsub("\\»", "", .) %>% 
  gsub("\\([0-9]+\\)", "", .) %>%
  gsub("\\s+"," ", .) %>% 
  gsub(" \\| ", ", ", .)

df2$X13 <- df2$X13 %>% 
  gsub("Genres:\n", "", .) %>% 
  gsub("\\s+"," ", .) %>% 
  gsub(" \\| ", ", ", .) %>% 
  gsub("\\|", "", .)

# Build df
df1 <- select(df1, X1, X2, X13)
df2 <- select(df2, X1, X2, X13)

desc <- rbind(df1, df2)

description <- desc %>% 
  rename(description = X1, 
         plot_keywords = X2,
         genres = X13)
```


```{r build df for metascore and other data}
# Function to scrape metascore and other data
getmeta <- function(url){
  page <- read_html(url)
  nodes <- html_nodes(page, '#titleDetails')
  cast <- html_text(nodes, trim = TRUE)
  return(cast)
}

# Use lapply() to apply the getmeta() function to each URL 
meta1 <- lapply(m_urls1, getmeta)
meta2 <- lapply(m_urls2, getmeta)

# Create a df that fills every list shorter than 11 with NA's
meta <- data.frame(cbind(append(meta1, meta2)))
colnames(meta) <- "X"

# Extract the information snippets
meta$country <- gsub(".*Country\\s*|Language.*", "", meta$X)
meta$profit <- gsub(".*Gross\\:s*|See.*", "", meta$X)
meta$production <- gsub(".*Production Co\\:s*|See.*", "", meta$X)
meta$runtime <- gsub(".*Runtime\\:s*|min.*", "", meta$X)

# Cleaning
meta$country <- meta$country %>% 
  gsub("\\s+"," ", .) %>% 
  gsub("\\|", ",", .) %>% 
  gsub("\\:", "", .)

meta$profit <- gsub("\\$*", "", meta$profit)
meta$runtime <- gsub("\\s+"," ", meta$runtime)

meta[which(grepl("Edit", meta$profit)), 3] <- NA
meta[which(grepl("Edit", meta$runtime)), 5] <- NA
meta[32, 4] <- "See-Saw Films, British Film Institute (BFI), BBC Films"

meta <- select(meta, -X)
```


```{r build dataframe for title and year}
# Scrape HTML content from a given URL
url1 <- read_html("https://www.imdb.com/search/title/?year=2020&title_type=feature&")
url2 <- read_html("https://www.imdb.com/search/title/?title_type=feature&year=2020-01-01,2020-12-31&start=51&ref_=adv_nxt")
# Call node based on xpath
title_nodes1 <- html_nodes(url1, ".text-muted.unbold , .lister-item-header a")
title_nodes2 <- html_nodes(url2, ".text-muted.unbold , .lister-item-header a")

# Strip the HTML tags and extract title and year 
titles1 <- html_text(title_nodes1)
titles2 <- html_text(title_nodes2)

# Create df 
titles <- as.data.frame(matrix(c(titles1, titles2), ncol = 2, byrow = TRUE), 
                      stringsAsFactors = FALSE)
colnames(titles) <- c("Title", "Year")

# Cleaning 
titles$Year <- titles$Year %>% 
  gsub("\\(", "",.) %>% 
  gsub("\\)", "", .) %>% 
  gsub("[A-Za-z]", "", .) %>% 
  gsub("\\s+", "", .)
```

```{r build df for rating}
# Function to scrape metascore and other data
getrating <- function(url){
  page <- read_html(url)
  nodes <- html_nodes(page, '.titleReviewBar , .ratings_wrapper')
  cast <- html_text(nodes, trim = TRUE)
  return(cast)
}

url_1 <- "https://www.imdb.com/search/title/?year=2020&title_type=feature&"
url_2 <- "https://www.imdb.com/search/title/?title_type=feature&year=2020-01-01,2020-12-31&start=51&ref_=adv_nxt"

# Get rating for both pages
rat1 <- lapply(url_2, getrating)
rat2 <- lapply(url_1, getrating)

# Build df
rating <- data.frame(cbind(append(rat1, rat2)))
colnames(rating) <- "X"
```



``` {r final}
# Combine df
total <- cbind(titles, description, meta)
write.xlsx(total, 'scraped_data.xlsx')
```