---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

# Info 

The current application uses data from the IMDb Charts, which are subject to change. The list itself is called "Popular Movies", and determined by an unidentified method by IMDb Users and sorted by Ranking. The list can be accessed here: https://www.imdb.com/chart/moviemeter/?ref_=nv_mv_mpm

# Outline 

After loading the libraries, the 100 most popular films are scraped by vectorization. The second data chunk is dedicated to retrieving some basic characteristics: rating, metascore, description, scoreline, plot keywords and genres. The third data chunk is dedicated to acquiring the movies' title and year of screening.

```{r message = FALSE}
library(rvest)
library(stringr)
library(dplyr)
library(openxlsx)
```

```{r scrape 100 most popular films based on rating}
# Scrape HTML content from a given URL
parsed_url <- read_html("https://www.imdb.com/chart/moviemeter/?ref_=nv_mv_mpm")

# Call node based on xpath
nodes <- html_nodes(parsed_url, "#main a")

# Create IMDb links to scrape through 
sub_urls <- html_attr(nodes, 'href') # extract parts of the link 
sub <- sub_urls[!is.na(sub_urls)] # remove NA
ub <- unique(sub) # remove duplicates
m_urls <- paste0('http://www.imdb.com', ub) # Concatenate with part that is missing from link

# Function to scrape description 
getdescription <- function(url){
  page <- read_html(url)
  nodes <- html_nodes(page, '.canwrap , #titleStoryLine p , .summary_text , .ratingValue span')
  cast <- html_text(nodes, trim = TRUE)
  return(cast)
}

# Use lapply() to apply the getdescription() function to each URL 
desc <- lapply(m_urls, getdescription)
```

```{r build dataframe for description}
# Get individual length of lists 
len <- sapply(desc,length)

# Get longest list 
n <- max(len)

# Get number of NA's to fill for lists shorter than longest
lenNA <- n - len

# Create a df that fills every list shorter than 11 with NA's
df <- data.frame(t(mapply(function(x,y) c(x, rep(NA, y)), desc, lenNA)))

# Cleaning

# Shift rows with missing information to the right 
df[which(grepl("[A-Za-z]", df$X1)), 4:8] <-  df[which(grepl("[A-Za-z]", df$X1)), 1:5]

# Fill missing information with NA
df[which(grepl("[A-Za-z]", df$X1)), 1:3] <- NA

df[which(grepl("Genres", df$X7)), 8] <-  df[which(grepl("Genres", df$X7)), 7]
df[which(grepl("Genres", df$X7)), 7] <- NA

# Clean columns
df$X7 <- df$X7 %>% 
  str_replace(., "Plot Keywords:", "") %>% 
  gsub("SeeAll\\([0-9]+\\)", "", .) %>% 
  gsub("See All", "", .) %>% 
  gsub("\\Â»", "", .) %>% 
  gsub("\\([0-9]+\\)", "", .) %>%
  gsub("\\s+"," ", .) %>% 
  gsub(" \\| ", ", ", .)

df$X8 <- df$X8 %>% 
  gsub("Genres:\n ", "", .) %>% 
  gsub("\\|", ",", .) %>% 
  gsub("\n", "", .)

description <- df %>% 
  select(-c(X6, X2, X3)) %>% 
  rename(rating = X1, 
         description = X4,
         storyline = X5,
         plot_keywords = X7,
         genres = X8)
```



```{r build dataframe for title and year}
# Scrape HTML content from a given URL
parsed_url <- read_html("https://www.imdb.com/chart/moviemeter/?ref_=nv_mv_mpm")

# Call node based on xpath
title_nodes <- html_nodes(parsed_url, "a+ .secondaryInfo , #main a")

# Strip the HTML tags and extract title and year 
titles <- html_text(title_nodes)

# Remove blanks 
col <- titles[!titles %in% c("", " ")]

# Create df 
col1 <- as.data.frame(matrix(col, ncol = 2, byrow = TRUE), 
                      stringsAsFactors = FALSE)

# Remove brackets 
col1[] <- lapply(col1, function(x) gsub("[][(),]", "", x))

# Define column names
colnames(col1) <- c("Title", "Year")

```

``` {r final}
# Combine df
total <- cbind(col1, description)
write.xlsx(total, 'scraped_data.xlsx')
```