---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

# Info 

The current application uses data from the IMDb Charts, which are subject to change. The list itself is called "Feature Film, Released between 2020-01-01 and 2020-12-31 (Sorted by Popularity Ascending)" and can be accessed here: https://www.imdb.com/search/title/?year=2020&title_type=feature&

# Outline 

After loading the libraries, two html lists for each page are created. What follows are code chunks for certain information snippets: description, profit and runtime, title and year, rating and metascore, actors and directors as well as votes. Sometimes, the information was available on the list, other times the information was to be found on the page of the movie itself. 

```{r message = FALSE}
library(rvest)
library(stringr)
library(dplyr)
library(openxlsx)
```

```{r scrape 100 most popular films based on rating}
# Page 1
# Scrape HTML content from a given URL
url1 <- read_html("https://www.imdb.com/search/title/?year=2020&title_type=feature&")
# Call node based on xpath
nodes1 <- html_nodes(url1, ".lister-item-header a")
# Create IMDb links to scrape through 
sub_urls1 <- html_attr(nodes1, 'href') # extract parts of the link 
m_urls1 <- paste0('http://www.imdb.com', sub_urls1) # Concatenate with part that is missing from link

# Page 2
# Scrape HTML content from a given URL
url2 <- read_html("https://www.imdb.com/search/title/?title_type=feature&year=2020-01-01,2020-12-31&start=51&ref_=adv_nxt")
# Call node based on xpath
nodes2 <- html_nodes(url2, ".lister-item-header a")
# Create IMDb links to scrape through 
sub_urls2 <- html_attr(nodes2, 'href') # extract parts of the link 
m_urls2 <- paste0('http://www.imdb.com', sub_urls2) # Concatenate with part that is missing from link
```

```{r description}
# Function to scrape description 
getdescription <- function(url){
  page <- read_html(url)
  nodes <- html_nodes(page, '.canwrap span , .see-more.canwrap')
  cast <- html_text(nodes, trim = TRUE)
  return(cast)
}

# Use lapply() to apply the getdescription() function to each URL 
desc1 <- lapply(m_urls1, getdescription)
desc2 <- lapply(m_urls2, getdescription)
# Get individual length of lists 
len1 <- sapply(desc1,length)
len2 <- sapply(desc2,length)
# Get longest list 
n1 <- max(len1)
n2 <- max(len2)
# Get number of NA's to fill for lists shorter than longest
lenNA1 <- n1 - len1
lenNA2 <- n2 - len2
# Create a df that fills every list shorter than 11 with NA's
df1 <- data.frame(t(mapply(function(x,y) c(x, rep(NA, y)), desc1, lenNA1)))
df2 <- data.frame(t(mapply(function(x,y) c(x, rep(NA, y)), desc2, lenNA2)))

# Correct inconsistencies 
# Table 1
df1[which(!(grepl("Plot", df1$X2))), 13] <- df1[which(!(grepl("Plot", df1$X2))), 2]
df1[which(!(grepl("Plot", df1$X2))), 2] <- NA
df1[which(grepl("Genres", df1$X5)), 13] <- df1[which(grepl("Genres", df1$X5)), 5]
df1[which(grepl("Genres", df1$X7)), 13] <- df1[which(grepl("Genres", df1$X7)), 7]
df1[which(grepl("Genres", df1$X9)), 13] <- df1[which(grepl("Genres", df1$X9)), 9]
df1[which(grepl("Genres", df1$X11)), 13] <- df1[which(grepl("Genres", df1$X11)), 11]

# Table 2
df2[which(!(grepl("Plot", df2$X2))), 13] <- df2[which(!(grepl("Plot", df2$X2))), 2]
df2[which(!(grepl("Plot", df2$X2))), 2] <- NA
df2[which(grepl("Genres", df2$X5)), 13] <- df2[which(grepl("Genres", df2$X5)), 5]
df2[which(grepl("Genres", df2$X7)), 13] <- df2[which(grepl("Genres", df2$X7)), 7]
df2[which(grepl("Genres", df2$X9)), 13] <- df2[which(grepl("Genres", df2$X9)), 9]


# Cleaning
df1$X2 <- df1$X2 %>% 
  gsub("Plot Keywords:", "", .) %>% 
  gsub("SeeAll\\([0-9]+\\)", "", .) %>% 
  gsub("See All", "", .) %>% 
  gsub("\\»", "", .) %>% 
  gsub("\\([0-9]+\\)", "", .) %>%
  gsub("\\s+"," ", .) %>% 
  gsub(" \\| ", ", ", .)

df1$X13 <- df1$X13 %>% 
  gsub("Genres:\n", "", .) %>% 
  gsub("\\s+"," ", .) %>% 
  gsub(" \\| ", ", ", .) %>% 
  gsub("\\|", "", .)

df2$X2 <- df2$X2 %>% 
  gsub("Plot Keywords:", "", .) %>% 
  gsub("SeeAll\\([0-9]+\\)", "", .) %>% 
  gsub("See All", "", .) %>% 
  gsub("\\»", "", .) %>% 
  gsub("\\([0-9]+\\)", "", .) %>%
  gsub("\\s+"," ", .) %>% 
  gsub(" \\| ", ", ", .)

df2$X13 <- df2$X13 %>% 
  gsub("Genres:\n", "", .) %>% 
  gsub("\\s+"," ", .) %>% 
  gsub(" \\| ", ", ", .) %>% 
  gsub("\\|", "", .)

# Build df
df1 <- select(df1, X1, X2, X13)
df2 <- select(df2, X1, X2, X13)

desc <- rbind(df1, df2)

description <- desc %>% 
  select(X1, X13) %>% 
  rename(Description = X1, 
         Genres = X13)
```


```{r profit and runtime}
# Function to scrape profit and runtime
getmeta <- function(url){
  page <- read_html(url)
  nodes <- html_nodes(page, '#titleDetails')
  cast <- html_text(nodes, trim = TRUE)
  return(cast)
}

# Use lapply() to apply the getmeta() function to each URL 
meta1 <- lapply(m_urls1, getmeta)
meta2 <- lapply(m_urls2, getmeta)

# Create a df that fills every list shorter than 11 with NA's
meta <- data.frame(cbind(append(meta1, meta2)))
colnames(meta) <- "X"

# Extract the information snippets
meta$country <- gsub(".*Country\\s*|Language.*", "", meta$X)
meta$profit <- gsub(".*Gross\\:s*|See.*", "", meta$X)
meta$production <- gsub(".*Production Co\\:s*|See.*", "", meta$X)
meta$runtime <- gsub(".*Runtime\\:s*|min.*", "", meta$X)

# Cleaning
meta$country <- meta$country %>% 
  gsub("\\s+"," ", .) %>% 
  gsub("\\|", ",", .) %>% 
  gsub("\\:", "", .)

meta$profit <- gsub("\\$*", "", meta$profit)
meta$runtime <- gsub("\\s+"," ", meta$runtime)

meta[which(grepl("Edit", meta$profit)), 3] <- NA
meta[which(grepl("Edit", meta$runtime)), 5] <- NA
meta[32, 4] <- "See-Saw Films, British Film Institute (BFI), BBC Films"

metadat <- meta %>% 
  select(profit, runtime) %>% 
  rename(Runtime..Minutes. = runtime,
         Revenue..Millions. = profit)

metadat$Revenue..Millions. <- as.numeric(as.factor(metadat$Revenue..Millions.))
metadat$Runtime..Minutes. <- as.integer(metadat$Runtime..Minutes.)
```


```{r title and year}
# Scrape HTML content from a given URL
url1 <- read_html("https://www.imdb.com/search/title/?year=2020&title_type=feature&")
url2 <- read_html("https://www.imdb.com/search/title/?title_type=feature&year=2020-01-01,2020-12-31&start=51&ref_=adv_nxt")
# Call node based on xpath
title_nodes1 <- html_nodes(url1, ".text-muted.unbold , .lister-item-header a")
title_nodes2 <- html_nodes(url2, ".text-muted.unbold , .lister-item-header a")

# Strip the HTML tags and extract title and year 
titles1 <- html_text(title_nodes1)
titles2 <- html_text(title_nodes2)

# Create df 
titles <- as.data.frame(matrix(c(titles1, titles2), ncol = 2, byrow = TRUE), 
                      stringsAsFactors = FALSE)
colnames(titles) <- c("Title", "Year")

# Cleaning 
titles$Year <- titles$Year %>% 
  gsub("\\(", "",.) %>% 
  gsub("\\)", "", .) %>% 
  gsub("[A-Za-z]", "", .) %>% 
  gsub("\\s+", "", .)

titles$Year <- as.integer(titles$Year)
```

```{r rating and metascore}
# Function to scrape Metascore and other data
getrating <- function(url){
  page <- read_html(url)
  nodes <- html_nodes(page, '.titleReviewBar , .ratings_wrapper')
  cast <- html_text(nodes, trim = TRUE)
  return(cast)
}

# Get rating for both pages
rat1 <- lapply(m_urls1, getrating)
rat2 <- lapply(m_urls2, getrating)

# Create df
rat <- as.data.frame(matrix(c(rat1, rat2), ncol = 1, byrow = TRUE), 
                      stringsAsFactors = FALSE)
colnames(rat) <- "x"

# Extract information 
rat$Rating <- as.numeric(str_extract(rat$x,"(\\d\\.\\d).*?"))
rat$Metascore <- str_extract(rat$x, "[[:punct:]]+\\s[[:punct:]][[:digit:]]*")

# Cleaning
rat$Metascore <- gsub('[[:punct:] ]+',' ', rat$Metascore)
rat$Metascore[rat$Metascore == " "] <- NA
rat$Metascore <- as.numeric(rat$Metascore)


rating <- rat %>% select(Rating, Metascore)
```

```{r actors directors}
# Call node based on xpath
actd1 <- html_nodes(url1, ".text-muted~ .text-muted+ p , .ratings-bar~ .text-muted+ p")
actd2 <- html_nodes(url2, ".text-muted~ .text-muted+ p , .ratings-bar~ .text-muted+ p")

# Strip the HTML tags and extract title and year 
actd_1 <- html_text(actd1)
actd_2 <- html_text(actd2)

# Create df 
actdir <- as.data.frame(matrix(c(actd_1, actd_2), ncol = 1, byrow = TRUE), 
                      stringsAsFactors = FALSE)
colnames(actdir) <- "X"

# Extract information 
actdir$Actors <- gsub(".*Director\\s*|Stars.*", "", actdir$X)
actdir$Director <- gsub(".*Stars\\s*", "", actdir$X)

# Cleaning 
actdir$Actors <- actdir$Actors %>% 
  gsub("\\s+"," ", .) %>% 
  gsub("\\|", ",", .) %>% 
  gsub("s\\:", "", .) %>% 
  gsub("\\:", "", .) %>% 
  gsub(" , ", "", .)
actdir$Director <- actdir$Director %>% 
  gsub("\\s+"," ", .) %>% 
  gsub("\\|", ",", .) %>% 
  gsub("\\:", "", .) %>% 
  gsub(" , ", "", .)

crew <- actdir %>% 
  select(Actors, Director)

```

```{r votes}
# Extract votes based on xpath
v1 <- html_text(html_nodes(url1, ".sort-num_votes-visible")) # 17 missing
v2 <- html_text(html_nodes(url2, ".sort-num_votes-visible")) # 60, 88, 91, 100

# Create df
votes <- data.frame(cbind(append(v1, v2)))
colnames(votes) <- "X"

# Extract information & clean 
votes$Votes <- str_extract(votes$X, "\\d+(?:,\\d+)?")
v <- select(votes, Votes)

# Insert NA's for missing information
v$names <- rownames(v)
missing <- list(Votes = c(NA, NA, NA, NA, NA), names = c(17, 60, 88, 91, 100))
vo <- rbind(v, missing)
vot <-arrange(vo, as.numeric(names))


vote <- vot %>% 
  select(Votes)
```


``` {r final}
# Combine df
total <- cbind(titles, rating, vote, description, metadat, crew)
write.xlsx(total, 'scraped_data.xlsx')
```